= Deploy a sample pod

Now that the everything is ready, we can run a sample workload.
Let's first see what we can and must add into the pod yaml to make it run in a VM.

NOTE: All **yaml** files in this section could be also created in the web UI by clicking on the **Import yaml** button in the top bar on the right (circled in red in the image below). Importing a yaml in this way corresponds to creating a `.yaml` file and applying it with `oc apply -f`. Because we are going to perform also other command-line operations, like fetching ARO credentials, we will perform all operations via command line. Note that this does not apply for `az` or `bash` (`cat`, `oc`) commands

image::05-bar.png[link=self, window=blank]

[#options]
== Available options

=== Mandatory options
In order to run a pod in a VM, it is mandatory to specify the `runtimeClassName` field in the pod `spec`. For peer-pods, the runtime class is called `kata-remote`.

[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
# ...
spec:
  runtimeClassName: kata-remote
# ...
----

=== Optionals

* Add an annotation to the pod-templated object to use a manually defined instance size or an automatic instance size:
+
[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: Standard_D32as_v5
# ...
----
+
Note that the `machine_type` must be one of the one specified in `AZURE_INSTANCE_SIZES` or `AZURE_INSTANCE_SIZE` in the OSC xref:02-configure-osc.adoc#pp-cm[ConfigMap]. By default, all instance types will be `AZURE_INSTANCE_SIZE`.

* Define the amount of memory available for the workload to use. The workload will run on an automatic instance size based on the amount of memory available.
+
[source,yaml,role=execute]
----
apiVersion: v1
kind: <Pod>
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: <vcpus>
    io.katacontainers.config.hypervisor.default_memory: <memory>
# ...
----

[#example]
== Hello world example
This is a sample yaml that runs an `hello-openshift` pod. This pod acts as server and outputs `"Hello Openshift!"` every time is reached.

. Create and apply the yaml file.
+
[source,sh,role=execute]
----
cat > sample-openshift.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: hello-openshift
  labels:
    app: hello-openshift
spec:
  runtimeClassName: kata-remote
  containers:
    - name: hello-openshift
      image: quay.io/openshift/origin-hello-openshift
      ports:
        - containerPort: 8888
      securityContext:
        privileged: false
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 1001
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault
---
kind: Service
apiVersion: v1
metadata:
  name: hello-openshift-service
  labels:
    app: hello-openshift
spec:
  selector:
    app: hello-openshift
  ports:
    - port: 8888
EOF

cat sample-openshift.yaml
----
+
[source,sh,role=execute]
----
oc apply -f sample-openshift.yaml
----

. Wait that the pod is created.
+
[source,sh,role=execute]
----
watch oc get pods/hello-openshift
----
+
The pod is ready when the `STATUS` is in `Running`.

. Now expose the pod to make it reachable:
+
[source,sh,role=execute]
----
oc expose service hello-openshift-service -l app=hello-openshift
APP_URL=$(oc get routes/hello-openshift-service -o jsonpath='{.spec.host}')
----

. And try to connect to the pod. It should print `Hello Openshift!`.
+
[source,sh,role=execute]
----
curl ${APP_URL}
----

[#verify]
== Verify that the pod is running in a VM
How to be sure that all what we did so far is actually running in a VM? There are several ways to check this.

. Via ARO web UI.
  * Go to the https://portal.azure.com/#browse/Microsoft.Compute%2FVirtualMachines/subscriptionId/{azure_subscription}[Azure VM web console,window=_blank] and login. Insert the RH email and proceed with login.
  * In the `Subscription` filter, pick {aro_sandbox_name}. In this example, it is `pool-01-344`.
+
image::06-subscription.png[link=self, window=blank]
  * Look at the various VMs. You will see there are:
    ** 3 masters VM (called _aro-cluster-{guid}-<random chars>-master-0/1/2_)
    ** 3 workers VM (called _aro-cluster-{guid}-<random chars>-worker-<region>-<random chars>_)
    ** 1 _bastion-{guid}_ VM, used internally by the workshop infrastructure. The console on the right is actually connected to this VM, and all commands are being performed from here.
    ** 1 **podvm-hello-openshift-<random chars>**. This is where the `hello-openshift` pod is actually running! Note also how the instance tyoe under `Size` column at the right side is not the same as the other VMs. It is indeed `Standard_D8as_v5`, as specified in the OSC xref:02-configure-osc.adoc#pp-cm[ConfigMap].
+
image::07-hello.png[link=self, window=blank]

. Via command line using `az`. Result and observations are same as the web UI.
+
[source,sh,role=execute]
----
az vm list --query "[].{Name:name, VMSize:hardwareProfile.vmSize}" --output table
----
+
Expected output:
+
[source,texinfo,subs="attributes"]
----
Name                                          VMSize
--------------------------------------------  ----------------
aro-cluster-q5hqf-xs7zb-master-0              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-master-1              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-master-2              Standard_D8s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl  Standard_D4s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus2-vt87j  Standard_D4s_v3
aro-cluster-q5hqf-xs7zb-worker-eastus3-6dzt4  Standard_D4s_v3
podvm-hello-openshift-c0311387                Standard_D8as_v5
bastion-q5hqf                                 Standard_DS1_v2
----

. By SSH'ing into the VM.
  * Recall `id_rsa` created when xref:02-configure-osc.adoc#pp-key[setting up the operator]. We will use that to log into the pod VM.
+
[source,sh,role=execute]
----
oc exec -it -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon -- bash
----
  * Get the pod VM private ip address:
    ** List all VMs
+
[source,sh,role=execute]
----
ARO_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}')

az vm list \
  --resource-group $ARO_RESOURCE_GROUP \
  --output table
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
Name                                          ResourceGroup    Location    Zones
--------------------------------------------  ---------------  ----------  -------
aro-cluster-q5hqf-xs7zb-master-0              aro-gqvj3nvq     eastus      1
aro-cluster-q5hqf-xs7zb-master-1              aro-gqvj3nvq     eastus      2
aro-cluster-q5hqf-xs7zb-master-2              aro-gqvj3nvq     eastus      3
aro-cluster-q5hqf-xs7zb-worker-eastus1-6rlsl  aro-gqvj3nvq     eastus      1
aro-cluster-q5hqf-xs7zb-worker-eastus2-vt87j  aro-gqvj3nvq     eastus      2
aro-cluster-q5hqf-xs7zb-worker-eastus3-6dzt4  aro-gqvj3nvq     eastus      3
podvm-hello-openshift-c0311387                aro-gqvj3nvq     eastus
----
    ** Get private ip of the **podvm-hello-openshift-<random_char>** VM (in this case, `podvm-hello-openshift-c0311387`):
+
[source,sh,role=execute]
----
VM_NAME=podvm-hello-openshift-c0311387
ARO_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}')

az vm list-ip-addresses --name $VM_NAME \
  --resource-group $ARO_RESOURCE_GROUP \
  --output table
----
+
Example output:
+
[source,texinfo,subs="attributes"]
----
VirtualMachine                  PrivateIPAddresses
------------------------------  --------------------
podvm-hello-openshift-c0311387  10.0.2.10
----
  * Log into the CAA pod (one of the three running in the worker nodes):
+
[source,sh,role=execute]
----
oc exec -it -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon -- bash
----
  * Once in the CAA pod, the `id_rsa` key is already embedded inside in `/root/.ssh/`. Simply log into the pod VM:
+
[source,sh,role=execute]
----
ssh peerpod@10.0.2.10
----
  * Inspect the VM. For example, check the ip address with `ip addr` and see how it differs from the original vm (type `exit` to log out), check the kernel version with `uname -r`, processes running with `ps -aux`, os-release with `cat /etc/os-release` and so on.
+
Example: check the kernel version (assumes you are in the CAA pod).
+
[source,sh,role=execute]
----
uname -r # get current worker kernel
ssh peerpod@10.0.2.10 uname -r
----
+
Expected output:
+
[source,texinfo,subs="attributes"]
----
# uname -r
4.18.0-372.64.1.el8_6.x86_64 # <----------------
# ssh peerpod@10.0.2.10 uname -r
The authenticity of host '10.0.2.10 (10.0.2.10)' can't be established.
ED25519 key fingerprint is SHA256:nsvcoaTXbJQ+z8QqZE6QHmmiD33s+e5/GPZ4UxXSmvo.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Failed to add the host to the list of known hosts (/root/.ssh/known_hosts).
5.14.0-362.8.1.el9_3.x86_64 # <----------------
----

[#destroy]
== Destroy the hello-openshift pod
The `hello-openshift` pod is no different from any other pod, therefore it can be destroyed just as the others (via command line, web ui, etc.). Behind the scenes, the operator will make sure that the created VM will also be completely deallocated.